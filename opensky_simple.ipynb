{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# OpenSky Network - Flight Tracker\n\nFetch real-time flight data using REST data source.\n\n**Available Regions:**\n- `EUROPE`, `NORTH_AMERICA`, `SOUTH_AMERICA`, `ASIA`, `AUSTRALIA`, `AFRICA`\n\n**Note:** Make sure to install the `pyspark-data-sources` package before running this notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nfrom pyspark.sql import SparkSession\nfrom pyspark_datasources import rest_api_call_csv, parse_array_response, parse_array_response_streaming, RestDataSource\nimport json\n\n# Use getOrCreate to work with Databricks cluster\nspark = SparkSession.builder.appName(\"OpenSky\").getOrCreate()\n\n# Define regions (from opensky.py)\nregions = {\n    \"EUROPE\": {\"lamin\": 35.0, \"lamax\": 72.0, \"lomin\": -25.0, \"lomax\": 45.0},\n    \"NORTH_AMERICA\": {\"lamin\": 7.0, \"lamax\": 72.0, \"lomin\": -168.0, \"lomax\": -60.0},\n    \"SOUTH_AMERICA\": {\"lamin\": -56.0, \"lamax\": 15.0, \"lomin\": -90.0, \"lomax\": -30.0},\n    \"ASIA\": {\"lamin\": -10.0, \"lamax\": 82.0, \"lomin\": 45.0, \"lomax\": 180.0},\n    \"AUSTRALIA\": {\"lamin\": -50.0, \"lamax\": -10.0, \"lomin\": 110.0, \"lomax\": 180.0},\n    \"AFRICA\": {\"lamin\": -35.0, \"lamax\": 37.0, \"lomin\": -20.0, \"lomax\": 52.0},\n}\n\n# Column names for OpenSky flight arrays\ncolumn_names = [\n    \"icao24\", \"callsign\", \"origin_country\", \"time_position\", \"last_contact\",\n    \"longitude\", \"latitude\", \"geo_altitude\", \"on_ground\", \"velocity\",\n    \"true_track\", \"vertical_rate\", \"sensors\", \"baro_altitude\",\n    \"squawk\", \"spi\", \"category\"\n]\n\nprint(\"✓ Ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Example - One-Time Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Choose region\nregion = \"NORTH_AMERICA\"\nbbox = regions[region]\n\n# Create input DataFrame\ninput_df = spark.createDataFrame([{\"region\": region, **bbox}])\ndisplay(input_df)\n\n# Fetch flights using CSV method (better for Databricks)\nurl = f\"https://opensky-network.org/api/states/all?lamin={{lamin}}&lamax={{lamax}}&lomin={{lomin}}&lomax={{lomax}}\"\nresponse = rest_api_call_csv(input_df, url=url, method=\"GET\", queryType=\"querystring\", partitions=\"1\")\n\n# Parse to individual flights\nflights = parse_array_response(response, array_path=\"states\", column_names=column_names, timestamp_field=\"time\")\n\nprint(f\"\\n✓ Found {flights.count()} flights\")\ndisplay(flights.select(\"region\", \"time\", \"icao24\", \"callsign\", \"origin_country\", \"latitude\", \"longitude\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Alternative: Polling with Batch Calls\n\nSince the OpenSky API returns array-based data (not object-based), we can use a simple loop with batch calls for continuous monitoring."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom datetime import datetime\n\n# Choose region\nregion = \"NORTH_AMERICA\"\nbbox = regions[region]\n\n# Create input DataFrame\ninput_df = spark.createDataFrame([{\"region\": region, **bbox}])\n\n# Polling loop - fetch every 10 seconds for 5 minutes\nprint(f\"Starting continuous monitoring of {region}...\")\nprint(\"Press 'Stop' in Databricks to end the monitoring\\n\")\n\nurl = f\"https://opensky-network.org/api/states/all?lamin={{lamin}}&lamax={{lamax}}&lomin={{lomin}}&lomax={{lomax}}\"\n\ntry:\n    for i in range(30):  # 30 iterations = 5 minutes at 10 seconds each\n        # Fetch flights\n        response = rest_api_call_csv(input_df, url=url, method=\"GET\", queryType=\"querystring\", partitions=\"1\")\n        flights = parse_array_response(response, array_path=\"states\", column_names=column_names, timestamp_field=\"time\")\n        \n        # Display results\n        count = flights.count()\n        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Poll #{i+1}: Found {count} flights\")\n        \n        if count > 0:\n            display(flights.select(\"time\", \"icao24\", \"callsign\", \"origin_country\", \"latitude\", \"longitude\").limit(10))\n        \n        # Wait before next poll (skip on last iteration)\n        if i < 29:\n            time.sleep(10)\n            \nexcept KeyboardInterrupt:\n    print(\"\\nMonitoring stopped by user\")\n    \nprint(\"\\n✓ Monitoring completed\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Streaming Example - Generic REST Data Source\n\nUses the generic REST data source with streaming. The key is to NOT extract individual records in the streaming reader, but return the whole response for parsing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Register REST data source\nspark.dataSource.register(RestDataSource)\n\n# Choose region\nregion = \"NORTH_AMERICA\"\nbbox = regions[region]\n\n# Create input DataFrame\ninput_df = spark.createDataFrame([{\"region\": region, **bbox}])\ninput_json = json.dumps([{\"region\": region, **bbox}])\n\nprint(f\"Setting up streaming for region: {region}\")\nprint(f\"Bounding box: {bbox}\\n\")\n\n# Configure streaming\n# IMPORTANT: Set dataField=\"\" to return the whole response as JSON string\nurl = \"https://opensky-network.org/api/states/all?lamin={lamin}&lamax={lamax}&lomin={lomin}&lomax={lomax}\"\n\nprint(\"Creating streaming DataFrame...\")\nstream_df = spark.readStream.format(\"rest\") \\\n    .option(\"url\", url) \\\n    .option(\"method\", \"GET\") \\\n    .option(\"streaming\", \"true\") \\\n    .option(\"inputData\", input_json) \\\n    .option(\"queryType\", \"querystring\") \\\n    .option(\"streamingInterval\", \"10\") \\\n    .option(\"offsetType\", \"timestamp\") \\\n    .option(\"offsetField\", \"time\") \\\n    .option(\"initialOffset\", \"0\") \\\n    .option(\"dataField\", \"\") \\\n    .load()\n\nprint(f\"✓ Stream DataFrame created\")\nprint(f\"Schema: {stream_df.schema}\\n\")\n\n# Parse array response using helper function\nprint(\"Parsing array response using parse_array_response_streaming...\")\nflights = parse_array_response_streaming(\n    stream_df, \n    array_path=\"states\", \n    column_names=column_names, \n    timestamp_field=\"time\"\n)\n\nprint(f\"✓ Flights DataFrame created\")\nprint(f\"Schema: {flights.schema}\\n\")\n\n# Select columns to display\nflights_display = flights.select(\n    \"region\", \"time\", \"icao24\", \"callsign\", \"origin_country\", \n    \"longitude\", \"latitude\", \"geo_altitude\", \"velocity\"\n)\n\nprint(\"Starting streaming query...\")\nprint(\"Polling every 10 seconds. Use 'Stop' button to stop.\\n\")\n\n# Start streaming\ndisplay(flights_display, streamName=\"opensky_flights\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}