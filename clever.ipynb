{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45f9189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/yipman/Downloads/github/pyspark-data-sources\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: mkdocstrings<0.29.0,>=0.28.0 in ./.venv/lib/python3.14/site-packages (from mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (0.28.3)\n",
      "Requirement already satisfied: pyarrow>=11.0.0 in ./.venv/lib/python3.14/site-packages (from pyspark-data-sources==0.1.10) (22.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in ./.venv/lib/python3.14/site-packages (from pyspark-data-sources==0.1.10) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib<2.0.0,>=1.3.1 in ./.venv/lib/python3.14/site-packages (from pyspark-data-sources==0.1.10) (1.3.1)\n",
      "Requirement already satisfied: Jinja2>=2.11.1 in ./.venv/lib/python3.14/site-packages (from mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (3.1.6)\n",
      "Requirement already satisfied: Markdown>=3.6 in ./.venv/lib/python3.14/site-packages (from mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (3.9)\n",
      "Requirement already satisfied: MarkupSafe>=1.1 in ./.venv/lib/python3.14/site-packages (from mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (3.0.3)\n",
      "Requirement already satisfied: mkdocs>=1.4 in ./.venv/lib/python3.14/site-packages (from mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (1.6.1)\n",
      "Requirement already satisfied: mkdocs-autorefs>=1.4 in ./.venv/lib/python3.14/site-packages (from mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (1.4.3)\n",
      "Requirement already satisfied: mkdocs-get-deps>=0.2 in ./.venv/lib/python3.14/site-packages (from mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (0.2.0)\n",
      "Requirement already satisfied: pymdown-extensions>=6.3 in ./.venv/lib/python3.14/site-packages (from mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (10.16.1)\n",
      "Requirement already satisfied: mkdocstrings-python>=1.16.2 in ./.venv/lib/python3.14/site-packages (from mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (1.16.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.14/site-packages (from requests<3.0.0,>=2.31.0->pyspark-data-sources==0.1.10) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.14/site-packages (from requests<3.0.0,>=2.31.0->pyspark-data-sources==0.1.10) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.14/site-packages (from requests<3.0.0,>=2.31.0->pyspark-data-sources==0.1.10) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.14/site-packages (from requests<3.0.0,>=2.31.0->pyspark-data-sources==0.1.10) (2025.10.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.venv/lib/python3.14/site-packages (from requests-oauthlib<2.0.0,>=1.3.1->pyspark-data-sources==0.1.10) (3.3.1)\n",
      "Requirement already satisfied: click>=7.0 in ./.venv/lib/python3.14/site-packages (from mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (8.3.0)\n",
      "Requirement already satisfied: ghp-import>=1.0 in ./.venv/lib/python3.14/site-packages (from mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (2.1.0)\n",
      "Requirement already satisfied: mergedeep>=1.3.4 in ./.venv/lib/python3.14/site-packages (from mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (1.3.4)\n",
      "Requirement already satisfied: packaging>=20.5 in ./.venv/lib/python3.14/site-packages (from mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (25.0)\n",
      "Requirement already satisfied: pathspec>=0.11.1 in ./.venv/lib/python3.14/site-packages (from mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (0.12.1)\n",
      "Requirement already satisfied: pyyaml-env-tag>=0.1 in ./.venv/lib/python3.14/site-packages (from mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.14/site-packages (from mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (6.0.3)\n",
      "Requirement already satisfied: watchdog>=2.0 in ./.venv/lib/python3.14/site-packages (from mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.14/site-packages (from ghp-import>=1.0->mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in ./.venv/lib/python3.14/site-packages (from mkdocs-get-deps>=0.2->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (4.5.0)\n",
      "Requirement already satisfied: griffe>=1.6.2 in ./.venv/lib/python3.14/site-packages (from mkdocstrings-python>=1.16.2->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (1.14.0)\n",
      "Requirement already satisfied: colorama>=0.4 in ./.venv/lib/python3.14/site-packages (from griffe>=1.6.2->mkdocstrings-python>=1.16.2->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.1->ghp-import>=1.0->mkdocs>=1.4->mkdocstrings<0.29.0,>=0.28.0->mkdocstrings[python]<0.29.0,>=0.28.0->pyspark-data-sources==0.1.10) (1.17.0)\n",
      "Building wheels for collected packages: pyspark-data-sources\n",
      "  Building editable for pyspark-data-sources (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark-data-sources: filename=pyspark_data_sources-0.1.10-py3-none-any.whl size=8029 sha256=2d6eabca84aeead22c0af78dade66bdce2871fc390f986918e146564f1947ab5\n",
      "  Stored in directory: /private/var/folders/fz/zkvh37qx3kdcv_8w1sw8kckm0000gn/T/pip-ephem-wheel-cache-hce9uasv/wheels/93/05/37/32e83f585fb327acc4004f3f00a246d80c6c6969be9130dc9f\n",
      "Successfully built pyspark-data-sources\n",
      "Installing collected packages: pyspark-data-sources\n",
      "  Attempting uninstall: pyspark-data-sources\n",
      "    Found existing installation: pyspark-data-sources 0.1.10\n",
      "    Uninstalling pyspark-data-sources-0.1.10:\n",
      "      Successfully uninstalled pyspark-data-sources-0.1.10\n",
      "Successfully installed pyspark-data-sources-0.1.10\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Clever API - Fetch All Schools, Teachers, and Students\n",
    "\n",
    "This notebook fetches:\n",
    "1. ALL schools in the district\n",
    "2. ALL teachers for each school\n",
    "3. ALL students for each school\n",
    "4. Flattens nested JSON responses\n",
    "5. Saves everything as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/26 21:28:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark_datasources import rest_api_call, flatten_json_response\n",
    "import ast\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CleverAPIFetchAll\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Your Clever API token (Broward County Public Schools)\n",
    "CLEVER_TOKEN = \"5e861ed16121bc9e31333257ab8673eea5b66146\"\n",
    "\n",
    "# Create dummy input for GET requests\n",
    "dummy_input = spark.createDataFrame([{\"placeholder\": \"dummy\"}])\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "## Step 1: Fetch ALL Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fetch_schools",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ALL schools...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py:161: DeprecationWarning: This process (pid=51796) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "Exception ignored while finalizing file <_io.BufferedWriter name=5>:            \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fetched 291 schools\n",
      "+------------------------+------------------------------+\n",
      "|school_id               |school_name                   |\n",
      "+------------------------+------------------------------+\n",
      "|51defb2054c6691246000001|LLOYD ESTATES ELEMENTARY      |\n",
      "|51defb2054c6691246000002|NOVA BLANCHE FORMAN ELEMENTARY|\n",
      "|51defb2054c6691246000003|LAUDERHILL P. T. ELEMENTARY   |\n",
      "|51defb2054c6691246000004|HOLLYWOOD PARK ELEMENTARY     |\n",
      "|51defb2054c6691246000005|JAMES S. HUNT ELEMENTARY      |\n",
      "|51defb2054c6691246000006|CORAL COVE ACADEMY OF THE ARTS|\n",
      "|51defb2054c6691246000007|CORAL SPRINGS ELEMENTARY      |\n",
      "|51defb2054c6691246000008|PEMBROKE LAKES ELEMENTARY     |\n",
      "|51defb2054c6691246000009|WESTCHESTER ELEMENTARY        |\n",
      "|51defb2054c669124600000a|MAPLEWOOD ELEMENTARY          |\n",
      "+------------------------+------------------------------+\n",
      "only showing top 10 rows\n",
      "Total schools: 291\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching ALL schools...\")\n",
    "\n",
    "# Use rest_api_call - the recommended PySpark Data Source approach\n",
    "schools_response = rest_api_call(\n",
    "    dummy_input,\n",
    "    url=\"https://api.clever.com/v3.0/schools?limit=10000\",\n",
    "    method=\"GET\",\n",
    "    authType=\"Bearer\",\n",
    "    oauthToken=CLEVER_TOKEN,\n",
    "    queryType=\"inline\",\n",
    "    partitions=\"1\"\n",
    ")\n",
    "\n",
    "# Parse the output column to extract school data\n",
    "school_output = schools_response.select(\"output\").first()[\"output\"]\n",
    "school_data = ast.literal_eval(school_output)\n",
    "\n",
    "# Extract schools\n",
    "all_schools = []\n",
    "for school in school_data.get(\"data\", []):\n",
    "    school_info = school.get(\"data\", {})\n",
    "    all_schools.append({\n",
    "        \"school_id\": school_info.get(\"id\"),\n",
    "        \"school_name\": school_info.get(\"name\"),\n",
    "    })\n",
    "\n",
    "print(f\"✓ Fetched {len(all_schools)} schools\")\n",
    "\n",
    "# Convert to DataFrame - this will be our input for teachers and students\n",
    "schools_df = spark.createDataFrame(all_schools)\n",
    "schools_df.show(10, truncate=False)\n",
    "print(f\"Total schools: {schools_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "## Step 2: Fetch ALL Teachers for ALL Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fetch_teachers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ALL teachers using schools DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/26 21:28:51 WARN DataSourceManager: The data source rest replaced a previously registered data source.\n",
      "25/10/26 21:29:34 WARN TaskSetManager: Stage 11 contains a task of very large size (1247 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fetched teachers for all schools!\n",
      "Teachers response has 291 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching ALL teachers using schools DataFrame...\")\n",
    "\n",
    "# Use schools_df as input - rest_api_call will make one API call per school\n",
    "teachers_response = rest_api_call(\n",
    "    schools_df,\n",
    "    url=\"https://api.clever.com/v3.0/schools/{school_id}/users?role=teacher&limit=10000\",\n",
    "    method=\"GET\",\n",
    "    authType=\"Bearer\",\n",
    "    oauthToken=CLEVER_TOKEN,\n",
    "    queryType=\"inline\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Fetched teachers for all schools!\")\n",
    "print(f\"Teachers response has {teachers_response.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "## Step 3: Fetch ALL Students for ALL Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fetch_students",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ALL students using schools DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/26 21:37:58 WARN DataSourceManager: The data source rest replaced a previously registered data source.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fetched students for all schools!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/26 21:39:36 WARN TaskSetManager: Stage 18 contains a task of very large size (32617 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/10/26 21:39:36 ERROR Executor: Exception in task 6.0 in stage 18.0 (TID 72)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
      "\t... 25 more\n",
      "25/10/26 21:39:36 ERROR Executor: Exception in task 3.0 in stage 18.0 (TID 69)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
      "\t... 25 more\n",
      "25/10/26 21:39:36 WARN TaskSetManager: Lost task 3.0 in stage 18.0 (TID 69) (mac.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
      "\t... 25 more\n",
      "\n",
      "25/10/26 21:39:36 ERROR TaskSetManager: Task 3 in stage 18.0 failed 1 times; aborting job\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "    code = worker(sock, authenticated)\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 81, in worker\n",
      "    worker_main(infile, outfile)\n",
      "    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2068, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "       ~~~~~~~~^^^^^^^^\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "    code = worker(sock, authenticated)\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 81, in worker\n",
      "    worker_main(infile, outfile)\n",
      "    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2068, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "       ~~~~~~~~^^^^^^^^\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "25/10/26 21:39:37 WARN TaskSetManager: Lost task 7.0 in stage 18.0 (TID 73) (mac.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 18.0 failed 1 times, most recent failure: Lost task 3.0 in stage 18.0 (TID 69) (mac.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/10/26 21:39:37 WARN TaskSetManager: Lost task 2.0 in stage 18.0 (TID 68) (mac.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 18.0 failed 1 times, most recent failure: Lost task 3.0 in stage 18.0 (TID 69) (mac.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/10/26 21:39:37 WARN TaskSetManager: Lost task 5.0 in stage 18.0 (TID 71) (mac.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 18.0 failed 1 times, most recent failure: Lost task 3.0 in stage 18.0 (TID 69) (mac.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "Exception ignored while finalizing file <_io.BufferedWriter name=5>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Exception ignored while finalizing file <_io.BufferedWriter name=5>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "25/10/26 21:39:37 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 66) (mac.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 18.0 failed 1 times, most recent failure: Lost task 3.0 in stage 18.0 (TID 69) (mac.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/10/26 21:39:37 WARN TaskSetManager: Lost task 1.0 in stage 18.0 (TID 67) (mac.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 18.0 failed 1 times, most recent failure: Lost task 3.0 in stage 18.0 (TID 69) (mac.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/10/26 21:39:37 WARN TaskSetManager: Lost task 4.0 in stage 18.0 (TID 70) (mac.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 18.0 failed 1 times, most recent failure: Lost task 3.0 in stage 18.0 (TID 69) (mac.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n",
      "\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n",
      "\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n",
      "\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "Exception ignored while finalizing file <_io.BufferedWriter name=5>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Exception ignored while finalizing file <_io.BufferedWriter name=5>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Exception ignored while finalizing file <_io.BufferedWriter name=5>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Exception ignored while finalizing file <_io.BufferedWriter name=5>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o220.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 18.0 failed 1 times, most recent failure: Lost task 3.0 in stage 18.0 (TID 69) (mac.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: No buffer space available\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: No buffer space available\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      4\u001b[39m students_response = rest_api_call(\n\u001b[32m      5\u001b[39m     schools_df,\n\u001b[32m      6\u001b[39m     url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.clever.com/v3.0/schools/\u001b[39m\u001b[38;5;132;01m{school_id}\u001b[39;00m\u001b[33m/users?role=student&limit=10000\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     queryType=\u001b[33m\"\u001b[39m\u001b[33minline\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Fetched students for all schools!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStudents response has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mstudents_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o220.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 18.0 failed 1 times, most recent failure: Lost task 3.0 in stage 18.0 (TID 69) (mac.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: No buffer space available\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: No buffer space available\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:81)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:58)\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:542)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching ALL students using schools DataFrame...\")\n",
    "\n",
    "# Use schools_df as input - rest_api_call will make one API call per school\n",
    "students_response = rest_api_call(\n",
    "    schools_df,\n",
    "    url=\"https://api.clever.com/v3.0/schools/{school_id}/users?role=student&limit=10000\",\n",
    "    method=\"GET\",\n",
    "    authType=\"Bearer\",\n",
    "    oauthToken=CLEVER_TOKEN,\n",
    "    queryType=\"inline\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Fetched students for all schools!\")\n",
    "print(f\"Students response has {students_response.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "## Step 4: Flatten Nested JSON Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flatten_teachers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening teachers data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 19:03:02 WARN TaskSetManager: Stage 19 contains a task of very large size (1047 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Flattened 8490 individual teachers\n",
      "Columns: ['created', 'district', 'email', 'id', 'last_modified', 'name_first', 'name_last', 'roles_teacher_credentials_district_username', 'roles_teacher_legacy_id', 'roles_teacher_school', 'roles_teacher_schools', 'roles_teacher_sis_id', 'roles_teacher_state_id', 'roles_teacher_teacher_number', 'roles_teacher_title', 'school_id', 'school_name', 'name_middle']\n",
      "+------------------------+------------------------+----------------------------------------------------------------+------------------------+------------------------+----------+-----------------------------------+-------------------------------------------+------------------------+------------------------+------------------------+--------------------+----------------------+----------------------------+-------------------+------------------------+-----------+-----------+\n",
      "|created                 |district                |email                                                           |id                      |last_modified           |name_first|name_last                          |roles_teacher_credentials_district_username|roles_teacher_legacy_id |roles_teacher_school    |roles_teacher_schools   |roles_teacher_sis_id|roles_teacher_state_id|roles_teacher_teacher_number|roles_teacher_title|school_id               |school_name|name_middle|\n",
      "+------------------------+------------------------+----------------------------------------------------------------+------------------------+------------------------+----------+-----------------------------------+-------------------------------------------+------------------------+------------------------+------------------------+--------------------+----------------------+----------------------------+-------------------+------------------------+-----------+-----------+\n",
      "|2021-03-10T03:18:12.702Z|5940d0b58ec81e0001541ef3|alexander.keihanaikukauakahihuliheekahaunaele.139359@example.com|60482973a24d9900a0a49c55|2021-03-10T03:18:12.702Z|Alexander |Keihanaikukauakahihuliheekahaunaele|                                           |5940d26c480568ae7e003f7e|5940d254203e37907e000100|5940d254203e37907e000100|139359              |                      |139359                      |Teacher            |5940d254203e37907e0000f8|Cowlgiant  |NULL       |\n",
      "|2021-03-10T03:18:12.704Z|5940d0b58ec81e0001541ef3|liam.jinglehimerschmidt.121819@example.com                      |60482973a24d9900a0a49c56|2021-03-10T03:18:12.704Z|Liam      |Jinglehimerschmidt                 |                                           |5940d26a480568ae7e003986|5940d254203e37907e000100|5940d254203e37907e000100|121819              |53628547              |121819                      |Teacher            |5940d254203e37907e0000f8|Cowlgiant  |NULL       |\n",
      "|2021-03-10T03:18:12.705Z|5940d0b58ec81e0001541ef3|noah.jones.124563@example.com                                   |60482973a24d9900a0a49c57|2021-03-10T03:18:12.705Z|Noah      |Jones                              |                                           |5940d26a480568ae7e0039c9|5940d254203e37907e000100|5940d254203e37907e000100|124563              |50567344              |124563                      |                   |5940d254203e37907e0000f8|Cowlgiant  |NULL       |\n",
      "|2021-03-10T03:18:12.707Z|5940d0b58ec81e0001541ef3|kistiñe.jinglehimer'schmidt.124706@example.com                  |60482973a24d9900a0a49c58|2021-03-10T03:18:12.707Z|Kistiñe   |Jinglehimer'schmidt                |                                           |5940d26a480568ae7e0039f8|5940d254203e37907e000100|5940d254203e37907e000100|124706              |51101279              |124706                      |Teacher            |5940d254203e37907e0000f8|Cowlgiant  |NULL       |\n",
      "|2021-03-10T03:18:12.710Z|5940d0b58ec81e0001541ef3|matthew.wilson.125221@example.com                               |60482973a24d9900a0a49c59|2021-03-10T03:18:12.710Z|Matthew   |Wilson                             |                                           |5940d26a480568ae7e003a71|5940d254203e37907e000100|5940d254203e37907e000100|125221              |51645781              |125221                      |                   |5940d254203e37907e0000f8|Cowlgiant  |NULL       |\n",
      "+------------------------+------------------------+----------------------------------------------------------------+------------------------+------------------------+----------+-----------------------------------+-------------------------------------------+------------------------+------------------------+------------------------+--------------------+----------------------+----------------------------+-------------------+------------------------+-----------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored while finalizing file <_io.BufferedWriter name=5>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "print(\"Flattening teachers data...\")\n",
    "\n",
    "# Use flatten_json_response to extract and flatten the nested \"data\" array\n",
    "# Clever API returns: {\"data\": [{\"data\": {...teacher fields...}}, ...]}\n",
    "# fully_flatten=True recursively flattens ALL nested dicts into columns (default)\n",
    "# This converts name:{first, last, middle} -> name_first, name_last, name_middle\n",
    "teachers_df = flatten_json_response(\n",
    "    teachers_response,\n",
    "    json_path=\"data\",           # Extract the \"data\" array\n",
    "    flatten_nested_key=\"data\",  # Each item has nested \"data\" object to flatten\n",
    "    fully_flatten=True          # Recursively flatten all nested structures\n",
    ")\n",
    "\n",
    "print(f\"✓ Flattened {teachers_df.count()} individual teachers\")\n",
    "print(f\"Columns: {teachers_df.columns}\")\n",
    "teachers_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flatten_students",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening students data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 19:03:04 WARN TaskSetManager: Stage 24 contains a task of very large size (5790 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/10/24 19:03:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/10/24 19:03:20 WARN TaskSetManager: Stage 25 contains a task of very large size (2454 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Flattened 56908 individual students\n",
      "Columns: ['created', 'district', 'email', 'id', 'last_modified', 'name_first', 'name_last', 'name_middle', 'roles_student_credentials_district_username', 'roles_student_dob', 'roles_student_email', 'roles_student_enrollments', 'roles_student_gender', 'roles_student_grade', 'roles_student_graduation_year', 'roles_student_hispanic_ethnicity', 'roles_student_location_address', 'roles_student_location_city', 'roles_student_location_state', 'roles_student_location_zip', 'roles_student_race', 'roles_student_school', 'roles_student_schools', 'roles_student_sis_id', 'roles_student_state_id', 'roles_student_student_number', 'school_id', 'school_name']\n",
      "+------------------------+------------------------+-------------------------------------------+------------------------+------------------------+----------+------------------+------------+-------------------------------------------+-----------------+-------------------------------------------+-------------------------+--------------------+-------------------+-----------------------------+--------------------------------+------------------------------+---------------------------+----------------------------+--------------------------+------------------+------------------------+------------------------+--------------------+----------------------+----------------------------+------------------------+-------------+\n",
      "|created                 |district                |email                                      |id                      |last_modified           |name_first|name_last         |name_middle |roles_student_credentials_district_username|roles_student_dob|roles_student_email                        |roles_student_enrollments|roles_student_gender|roles_student_grade|roles_student_graduation_year|roles_student_hispanic_ethnicity|roles_student_location_address|roles_student_location_city|roles_student_location_state|roles_student_location_zip|roles_student_race|roles_student_school    |roles_student_schools   |roles_student_sis_id|roles_student_state_id|roles_student_student_number|school_id               |school_name  |\n",
      "+------------------------+------------------------+-------------------------------------------+------------------------+------------------------+----------+------------------+------------+-------------------------------------------+-----------------+-------------------------------------------+-------------------------+--------------------+-------------------+-----------------------------+--------------------------------+------------------------------+---------------------------+----------------------------+--------------------------+------------------+------------------------+------------------------+--------------------+----------------------+----------------------------+------------------------+-------------+\n",
      "|2021-03-10T03:17:05.731Z|5940d0b58ec81e0001541ef3|abigail.wilson.322712@example.com          |5940d2d57a70493c1401387b|2021-03-10T03:17:05.731Z|Abigail   |Wilson            |Weaselbrown |                                           |9/23/2003        |abigail.wilson.322712@example.com          |                         |F                   |6                  |                             |                                |                              |                           |                            |                          |                  |5940d254203e37907e0000f0|5940d254203e37907e0000f0|322712              |1049119525            |322712                      |5940d254203e37907e0000f0|Slothgreat   |\n",
      "|2021-03-10T03:16:26.920Z|5940d0b58ec81e0001541ef3|jacob.jinglehimerschmidt.238045@example.com|5940d2ae7a70493c1400ecae|2021-03-10T03:16:26.920Z|Jacob     |Jinglehimerschmidt|Yakquasar   |                                           |8/8/1997         |jacob.jinglehimerschmidt.238045@example.com|                         |M                   |11                 |                             |                                |                              |                           |                            |                          |                  |5940d254203e37907e0000f8|5940d254203e37907e0000f8|238045              |1078243616            |238045                      |5940d254203e37907e0000f4|Bunnyflicker |\n",
      "|2021-03-10T03:16:33.826Z|5940d0b58ec81e0001541ef3|NULL                                       |5940d2b47a70493c1400fa25|2021-03-10T03:16:33.826Z|Chloe     |Martinez          |Cranehate   |                                           |9/22/1997        |NULL                                       |                         |M                   |12                 |                             |                                |                              |                           |                            |                          |                  |5940d254203e37907e0000f8|5940d254203e37907e0000f8|249789              |1039139617            |249789                      |5940d254203e37907e0000f4|Bunnyflicker |\n",
      "|2021-03-10T03:16:45.949Z|5940d0b58ec81e0001541ef3|NULL                                       |5940d2c17a70493c140111b2|2021-03-10T03:16:45.949Z|Alexander |Smith             |Spiritzircon|                                           |2/16/2003        |NULL                                       |                         |M                   |7                  |                             |                                |                              |                           |                            |90474                     |                  |5940d254203e37907e0000fa|5940d254203e37907e0000fa|295777              |1094082716            |295777                      |5940d254203e37907e0000f5|Spearchestnut|\n",
      "|2021-03-10T03:18:10.874Z|5940d0b58ec81e0001541ef3|NULL                                       |5940d30f7a70493c1401b7b7|2021-03-10T03:18:10.874Z|John-Paul |Martinez          |NULL        |                                           |3/15/2005        |NULL                                       |                         |F                   |4                  |                             |                                |                              |                           |                            |                          |                  |5940d254203e37907e0000fa|5940d254203e37907e0000fa|382928              |1082929522            |382928                      |5940d254203e37907e0000f5|Spearchestnut|\n",
      "+------------------------+------------------------+-------------------------------------------+------------------------+------------------------+----------+------------------+------------+-------------------------------------------+-----------------+-------------------------------------------+-------------------------+--------------------+-------------------+-----------------------------+--------------------------------+------------------------------+---------------------------+----------------------------+--------------------------+------------------+------------------------+------------------------+--------------------+----------------------+----------------------------+------------------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 19:03:20 WARN TaskSetManager: Stage 28 contains a task of very large size (2454 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Exception ignored while finalizing file <_io.BufferedWriter name=5>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yipman/Downloads/github/pyspark-data-sources/.venv/lib/python3.14/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "print(\"Flattening students data...\")\n",
    "\n",
    "# Use flatten_json_response to extract and flatten the nested \"data\" array\n",
    "# Clever API returns: {\"data\": [{\"data\": {...student fields...}}, ...]}\n",
    "# fully_flatten=True recursively flattens ALL nested dicts into columns (default)\n",
    "# This converts name:{first, last, middle} -> name_first, name_last, name_middle\n",
    "# and roles:{student:{...}} -> roles_student_dob, roles_student_gender, etc.\n",
    "students_df = flatten_json_response(\n",
    "    students_response,\n",
    "    json_path=\"data\",           # Extract the \"data\" array\n",
    "    flatten_nested_key=\"data\",  # Each item has nested \"data\" object to flatten\n",
    "    fully_flatten=True          # Recursively flatten all nested structures\n",
    ")\n",
    "\n",
    "print(f\"✓ Flattened {students_df.count()} individual students\")\n",
    "print(f\"Columns: {students_df.columns}\")\n",
    "students_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5",
   "metadata": {},
   "source": [
    "## Step 5: Save Flattened Data as CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_csv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Schools saved to output/schools.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 19:03:20 WARN TaskSetManager: Stage 30 contains a task of very large size (2536 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Teachers saved to output/teachers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 19:03:21 WARN TaskSetManager: Stage 31 contains a task of very large size (19539 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Students saved to output/students.csv\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Schools:  241\n",
      "Teachers: 8490\n",
      "Students: 56908\n",
      "\n",
      "All data has been flattened and saved as CSV files.\n",
      "Each CSV file contains one row per school/teacher/student.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 19:03:21 WARN TaskSetManager: Stage 38 contains a task of very large size (2454 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Save Schools\n",
    "schools_output = \"output/schools.csv\"\n",
    "schools_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(schools_output)\n",
    "print(f\"✓ Schools saved to {schools_output}\")\n",
    "\n",
    "# Save Teachers (flattened - one row per teacher)\n",
    "teachers_output = \"output/teachers.csv\"\n",
    "teachers_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(teachers_output)\n",
    "print(f\"✓ Teachers saved to {teachers_output}\")\n",
    "\n",
    "# Save Students (flattened - one row per student)\n",
    "students_output = \"output/students.csv\"\n",
    "students_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(students_output)\n",
    "print(f\"✓ Students saved to {students_output}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Schools:  {schools_df.count()}\")\n",
    "print(f\"Teachers: {teachers_df.count()}\")\n",
    "print(f\"Students: {students_df.count()}\")\n",
    "print(\"\\nAll data has been flattened and saved as CSV files.\")\n",
    "print(\"Each CSV file contains one row per school/teacher/student.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
